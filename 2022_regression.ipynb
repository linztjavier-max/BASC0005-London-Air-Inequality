{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linztjavier-max/BASC0005-London-Air-Inequality/blob/main/2022_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8652zJ3yxq4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://data.london.gov.uk/download/v8pow/87e880c2-34bd-4d86-8895-e8c5344f358e/traffic-flow-borough.xlsx\"\n",
        "\n",
        "cars = pd.read_excel(url, sheet_name=\"Traffic Flows - Cars\")\n",
        "allv = pd.read_excel(url, sheet_name=\"Traffic Flows - All vehicles\")\n",
        "\n",
        "def clean_year(col):\n",
        "    try:\n",
        "        return int(col.split()[0])\n",
        "    except:\n",
        "        return col\n",
        "\n",
        "cars.columns = [clean_year(c) for c in cars.columns]\n",
        "allv.columns = [clean_year(c) for c in allv.columns]\n",
        "\n",
        "years = list(range(2019, 2023))\n",
        "\n",
        "cars_df = cars[[\"LA Code\", \"Local Authority\"] + years]\n",
        "vehicles_df = allv[[\"LA Code\", \"Local Authority\"] + years]"
      ],
      "metadata": {
        "id": "2JU9awubbxz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traffic Flow Car Output\n",
        "cars_df"
      ],
      "metadata": {
        "id": "IsTOv-N30yWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traffic Flow All Vehicle Output\n",
        "vehicles_df"
      ],
      "metadata": {
        "id": "69nEWPZT10nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# data in population\n",
        "df_path_pop = (\n",
        "    \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/\"\n",
        "    \"populationandmigration/populationestimates/datasets/\"\n",
        "    \"populationestimatesforukenglandandwalesscotlandandnorthernireland/\"\n",
        "    \"mid2024/mye24tablesuk.xlsx\"\n",
        ")\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "response = requests.get(df_path_pop, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "pop_df = pd.read_excel(BytesIO(response.content), sheet_name=\"MYE5\", header=7)\n"
      ],
      "metadata": {
        "id": "mzRzj_IL3L7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec52d159"
      },
      "source": [
        "#Output pandas in population\n",
        "pop_filtered_df = pop_df.iloc[201:234]\n",
        "def clean_pop_df_column_name(col_name):\n",
        "    if isinstance(col_name, str) and col_name.startswith('Mid-'):\n",
        "        try:\n",
        "            return int(col_name.replace('Mid-', ''))\n",
        "        except ValueError:\n",
        "            return col_name # Return original if conversion fails\n",
        "    return col_name\n",
        "\n",
        "pop_filtered_df.columns = [clean_pop_df_column_name(col) for col in pop_filtered_df.columns]\n",
        "pop_filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d560994"
      },
      "source": [
        "#Output pandas in population\n",
        "pop_filtered_df = pop_df.iloc[201:234]\n",
        "def clean_pop_df_column_name(col_name):\n",
        "    if isinstance(col_name, str) and col_name.startswith('Mid-'):\n",
        "        try:\n",
        "            return int(col_name.replace('Mid-', ''))\n",
        "        except ValueError:\n",
        "            return col_name # Return original if conversion fails\n",
        "    return col_name\n",
        "\n",
        "pop_filtered_df.columns = [clean_pop_df_column_name(col) for col in pop_filtered_df.columns]\n",
        "\n",
        "desired_columns = [\n",
        "    'Code',\n",
        "    'Name',\n",
        "    'Geography',\n",
        "    'Area (sq km)',\n",
        "    'Estimated Population mid-2022',\n",
        "    '2022 people per sq. km',\n",
        "    'Estimated Population mid-2019',\n",
        "    '2019 people per sq. km'\n",
        "]\n",
        "\n",
        "pop_selected_columns_df = pop_filtered_df[desired_columns]\n",
        "display(pop_selected_columns_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data in earnings\n",
        "df_path_earnings = \"https://data.london.gov.uk/download/2z0rk/1686ef1c-b169-442d-8877-e7e49788f668/earnings-residence-borough.xlsx\"\n",
        "\n",
        "earnings_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\")\n"
      ],
      "metadata": {
        "id": "oIZbFITAA9Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88b14276"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\", header=None)\n",
        "\n",
        "# Extract the first two rows to be used as header information\n",
        "header_row0 = raw_df.iloc[0] # Contains years (e.g., 2002, NaN, 2003, NaN)\n",
        "header_row1 = raw_df.iloc[1] # Contains sub-headers (e.g., Code, Area, Pay (£), conf %)\n",
        "\n",
        "# Construct new column names by combining the year and sub-header\n",
        "new_columns = []\n",
        "current_year = None\n",
        "\n",
        "for i in range(len(header_row0)):\n",
        "    year_val = header_row0.iloc[i]\n",
        "    sub_header_val = header_row1.iloc[i]\n",
        "\n",
        "    if i < 2: # Handle the first two columns ('Code', 'Area') specifically\n",
        "        new_columns.append(str(year_val).strip())\n",
        "    elif pd.isna(year_val): # If year is NaN, it's a sub-header like 'conf %' under a year\n",
        "        if current_year is not None:\n",
        "            new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "        else:\n",
        "            # This case implies a NaN year_val without a preceding year, which shouldn't happen for data columns\n",
        "            new_columns.append(str(sub_header_val).strip()) # Fallback for safety\n",
        "    else: # Year value is present (e.g., 2002, 2003, ...)\n",
        "        current_year = int(year_val)\n",
        "        new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "\n",
        "# Create the earnings_clean DataFrame by taking data from the third row onwards\n",
        "# and assigning the newly constructed column names.\n",
        "earnings_clean = raw_df.iloc[2:].copy()\n",
        "earnings_clean.columns = new_columns\n",
        "earnings_clean = earnings_clean.reset_index(drop=True)\n",
        "\n",
        "# Remove all 'conf %' columns\n",
        "columns_to_drop = [col for col in earnings_clean.columns if 'conf %' in col]\n",
        "earnings_clean = earnings_clean.drop(columns=columns_to_drop)\n",
        "\n",
        "# Identify the columns for 'Pay (£)' for years 2011 to 2024\n",
        "years_to_keep = list(range(2019, 2023))\n",
        "pay_columns = [f\"{year} Pay (£)\" for year in years_to_keep]\n",
        "\n",
        "# Ensure 'Code' and 'Area' are always kept\n",
        "final_columns = ['Code', 'Area'] + pay_columns\n",
        "\n",
        "# Filter earnings_clean to retain only these selected columns\n",
        "earnings_clean = earnings_clean[final_columns]\n",
        "\n",
        "earnings_clean_df = earnings_clean.iloc[0:34]\n",
        "\n",
        "print(\"Cleaned earnings_clean DataFrame:\")\n",
        "earnings_clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Make pandas show EVERYTHING\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "\n",
        "def load_full_pm25_summary(zip_url):\n",
        "    \"\"\"\n",
        "    Downloads LAEI ZIP file and returns the FULL PM2.5 Summary sheet as a DataFrame\n",
        "    \"\"\"\n",
        "    # Download ZIP\n",
        "    response = requests.get(zip_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Open ZIP\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        # Find Excel file\n",
        "        excel_name = [f for f in z.namelist() if f.lower().endswith(\".xlsx\")][0]\n",
        "\n",
        "        # Load Excel\n",
        "        excel_bytes = io.BytesIO(z.read(excel_name))\n",
        "        df = pd.read_excel(excel_bytes, sheet_name=\"PM2.5 Summary\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "url_2019 = (\n",
        "    \"https://data.london.gov.uk/download/\"\n",
        "    \"london-atmospheric-emissions-inventory--laei--2019/\"\n",
        "    \"17d21cd1-892e-4388-9fea-b48c1b61ee3c/\"\n",
        "    \"LAEI-2019-Emissions-Summary-including-Forecast.zip\"\n",
        ")\n",
        "\n",
        "url_2022 = (\n",
        "    \"https://data.london.gov.uk/download/2lg5g/4ql/\"\n",
        "    \"LAEI2022-Emissions-Summary-Excel.zip\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pm25_2019_full = load_full_pm25_summary(url_2019)\n",
        "pm25_2022_full = load_full_pm25_summary(url_2022)\n"
      ],
      "metadata": {
        "id": "YJaeExkJ-Cj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_2022 = pm25_2022_full.iloc[[6, 77], 11:]\n",
        "\n",
        "result_2022"
      ],
      "metadata": {
        "id": "_DrvhPvC_ViT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_2019 = pm25_2019_full.iloc[[6, 71], 11:]\n",
        "\n",
        "result_2019"
      ],
      "metadata": {
        "id": "lPpY90Uq-taS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining \"result_2022\", \"2022 Pay (£)\" in \"earnings_clean_df\", \"2022 people per sq. km\" in \"pop_selected_columns_df\", and \"2022\" in \"vehicles_df\".\n"
      ],
      "metadata": {
        "id": "O94n6zrc05hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ddb675a"
      },
      "source": [
        "# Renaming \"Code\" to \"LA Code\" and \"2022 Pay (£)\" to \"Local Authority\", and filtering out rows where \"LA Code\" is null.\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "df_path_earnings = \"https://data.london.gov.uk/download/2z0rk/1686ef1c-b169-442d-8877-e7e49788f668/earnings-residence-borough.xlsx\"\n",
        "\n",
        "raw_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\", header=None)\n",
        "\n",
        "# Extracting the first two rows to be used as header information\n",
        "header_row0 = raw_df.iloc[0] # Contains years (e.g., 2002, NaN, 2003, NaN)\n",
        "header_row1 = raw_df.iloc[1] # Contains sub-headers (e.g., Code, Area, Pay (£), conf %)\n",
        "\n",
        "# Constructing new column names by combining the year and sub-header\n",
        "new_columns = []\n",
        "current_year = None\n",
        "\n",
        "for i in range(len(header_row0)):\n",
        "    year_val = header_row0.iloc[i]\n",
        "    sub_header_val = header_row1.iloc[i]\n",
        "\n",
        "    if i < 2: # Handling the first two columns ('Code', 'Area') specifically\n",
        "        new_columns.append(str(year_val).strip())\n",
        "    elif pd.isna(year_val): # If year is NaN, it's a sub-header like 'conf %' under a year\n",
        "        if current_year is not None:\n",
        "            new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "        else:\n",
        "            # This case implies a NaN year_val without a preceding year, which shouldn't happen for data columns\n",
        "            new_columns.append(str(sub_header_val).strip()) # Fallback for safety\n",
        "    else: # Year value is present (e.g., 2002, 2003, ...)\n",
        "        current_year = int(year_val)\n",
        "        new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "\n",
        "# Creating the \"earnings_clean\" dataframe by taking data from the third row onwards and assigning the newly constructed column names.\n",
        "earnings_clean = raw_df.iloc[2:].copy()\n",
        "earnings_clean.columns = new_columns\n",
        "earnings_clean = earnings_clean.reset_index(drop=True)\n",
        "\n",
        "# Removing all 'conf %' columns\n",
        "columns_to_drop = [col for col in earnings_clean.columns if 'conf %' in col]\n",
        "earnings_clean = earnings_clean.drop(columns=columns_to_drop)\n",
        "\n",
        "# Identifying the columns for 'Pay (£)' for years 2011 to 2024\n",
        "years_to_keep = list(range(2019, 2023))\n",
        "pay_columns = [f\"{year} Pay (£)\" for year in years_to_keep]\n",
        "\n",
        "# Ensuring that 'Code' and 'Area' are always kept\n",
        "final_columns = ['Code', 'Area'] + pay_columns\n",
        "\n",
        "# Filtering \"earnings_clean\" to retain only these selected columns\n",
        "earnings_clean = earnings_clean[final_columns]\n",
        "\n",
        "earnings_clean_df = earnings_clean.iloc[0:34]\n",
        "\n",
        "# Printing the first few rows of \"earnings_2022_df\"\n",
        "earnings_2022_df = earnings_clean_df[['Code', 'Area', '2022 Pay (£)']].copy()\n",
        "earnings_2022_df.rename(columns={'Code': 'LA Code', 'Area': 'Local Authority'}, inplace=True)\n",
        "earnings_2022_df.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Prepared 2022 earnings data:\")\n",
        "earnings_2022_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the population density data for 2022. - Renaming \"Code\" to \"LA Code\" and \"Name\" to \"Local Authority\" and removing any rows with missing \"LA Code\"\n",
        "population_2022_df = pop_selected_columns_df[['Code', 'Name', '2022 people per sq. km']].copy()\n",
        "population_2022_df.rename(columns={'Code': 'LA Code', 'Name': 'Local Authority'}, inplace=True)\n",
        "population_2022_df.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Prepared 2022 population data:\")\n",
        "population_2022_df.head()"
      ],
      "metadata": {
        "id": "cPWk0S8jklx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing traffic intensity data for 2022\n",
        "vehicles_2022_df = vehicles_df[['LA Code', 'Local Authority', 2022]].copy()\n",
        "vehicles_2022_df.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Prepared 2022 vehicles data:\")\n",
        "vehicles_2022_df.head()"
      ],
      "metadata": {
        "id": "Hqcm5w1Wknvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing PM2.5 air pollution data and making sure that only numeric values are present.\n",
        "import pandas as pd\n",
        "\n",
        "# Extracting borough names (excluding 'Row Labels')\n",
        "borough_names = result_2022.iloc[0, 1:].values\n",
        "# Extracting PM2.5 values (excluding 'Grand Total')\n",
        "pm25_values = result_2022.iloc[1, 1:].values\n",
        "\n",
        "pm25_2022_df = pd.DataFrame({\n",
        "    'Local Authority': borough_names,\n",
        "    '2022 PM2.5': pm25_values\n",
        "})\n",
        "\n",
        "# Filtering out specific local authority names that are not actual boroughs or are redundant\n",
        "pm25_2022_df = pm25_2022_df[~pm25_2022_df['Local Authority'].isin(['Non GLA', 'Grand Total', 'City', 'City of Westminster'])].copy()\n",
        "\n",
        "# Converting PM2.5 to numeric values\n",
        "pm25_2022_df['2022 PM2.5'] = pd.to_numeric(pm25_2022_df['2022 PM2.5'])\n",
        "\n",
        "# Maping out Local Authority names to match other dataframes where possible\n",
        "name_mapping = {\n",
        "    'Barking and Dagenham': 'Barking and Dagenham',\n",
        "    'Barnet': 'Barnet',\n",
        "    'Bexley': 'Bexley',\n",
        "    'Brent': 'Brent',\n",
        "    'Bromley': 'Bromley',\n",
        "    'Camden': 'Camden',\n",
        "    'Croydon': 'Croydon',\n",
        "    'Ealing': 'Ealing',\n",
        "    'Enfield': 'Enfield',\n",
        "    'Greenwich': 'Greenwich',\n",
        "    'Hackney': 'Hackney',\n",
        "    'Hammersmith and Fulham': 'Hammersmith and Fulham',\n",
        "    'Haringey': 'Haringey',\n",
        "    'Harrow': 'Harrow',\n",
        "    'Havering': 'Havering',\n",
        "    'Hillingdon': 'Hillingdon',\n",
        "    'Hounslow': 'Hounslow',\n",
        "    'Islington': 'Islington',\n",
        "    'Kensington and Chelsea': 'Kensington and Chelsea',\n",
        "    'Kingston': 'Kingston upon Thames',\n",
        "    'Lambeth': 'Lambeth',\n",
        "    'Lewisham': 'Lewisham',\n",
        "    'Merton': 'Merton',\n",
        "    'Newham': 'Newham',\n",
        "    'Redbridge': 'Redbridge',\n",
        "    'Richmond': 'Richmond upon Thames',\n",
        "    'Southwark': 'Southwark',\n",
        "    'Sutton': 'Sutton',\n",
        "    'Tower Hamlets': 'Tower Hamlets',\n",
        "    'Waltham Forest': 'Waltham Forest',\n",
        "    'Wandsworth': 'Wandsworth',\n",
        "    'City of London': 'City of London'\n",
        "}\n",
        "pm25_2022_df['Local Authority'] = pm25_2022_df['Local Authority'].replace(name_mapping)\n",
        "\n",
        "print(\"Prepared 2022 PM2.5 data:\")\n",
        "pm25_2022_df.head()"
      ],
      "metadata": {
        "id": "E-0D0crWkr8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging income, population density, traffic intensity, and air pollution data into one dataframe for 2022.\n",
        "merged_2022_data = pd.merge(earnings_2022_df, population_2022_df, on='Local Authority', how='outer')\n",
        "merged_2022_data = pd.merge(merged_2022_data, vehicles_2022_df, on='Local Authority', how='outer')\n",
        "merged_2022_data = pd.merge(merged_2022_data, pm25_2022_df, on='Local Authority', how='outer')\n",
        "\n",
        "# Ensuring 'LA Code' from earnings is used as primary, or population if earnings is missing\n",
        "# The first step is to clean the LA Code from \"earnings_202_df\", which might have a '00AA' format (placeholder format)and converting other LA Codes to object type to avoid dtype issues during merge.\n",
        "merged_2022_data['LA Code_x'] = merged_2022_data['LA Code_x'].replace({'00AA': 'E09000001', '00AB': 'E09000002', '00AC': 'E09000003', '00AD': 'E09000004', '00AE': 'E09000005', '00AF': 'E09000006', '00AG': 'E09000007', '00AH': 'E09000008', '00AJ': 'E09000009', '00AK': 'E09000010', '00AL': 'E09000011', '00AM': 'E09000012', '00AN': 'E09000013', '00AP': 'E09000014', '00AQ': 'E09000015', '00AR': 'E09000016', '00AS': 'E09000017', '00AT': 'E09000018', '00AU': 'E09000019', '00AW': 'E09000020', '00AX': 'E09000021', '00AY': 'E09000022', '00AZ': 'E09000023', '00BA': 'E09000024', '00BB': 'E09000025', '00BC': 'E09000026', '00BD': 'E09000027', '00BE': 'E09000028', '00BF': 'E09000029', '00BG': 'E09000030', '00BH': 'E09000031', '00BJ': 'E09000032', '00BK': 'E09000033'})\n",
        "\n",
        "merged_2022_data['LA Code'] = merged_2022_data['LA Code_x'].fillna(merged_2022_data['LA Code_y'])\n",
        "merged_2022_data = merged_2022_data.drop(columns=['LA Code_x', 'LA Code_y'])\n",
        "\n",
        "# Renaming columns for clarity\n",
        "merged_2022_data.rename(columns={\n",
        "    '2022 Pay (£)': '2022_Avg_Weekly_Earnings',\n",
        "    '2022 people per sq. km': '2022_Population_Density',\n",
        "    2022: '2022_Vehicle_Traffic'\n",
        "}, inplace=True)\n",
        "\n",
        "# Removing the 'LONDON' row from the population data as it is a region, not a borough.\n",
        "merged_2022_data = merged_2022_data[merged_2022_data['Local Authority'] != 'LONDON']\n",
        "\n",
        "# Removing rows where 'LA Code' is still NaN (non-borough entries that might have slipped through)\n",
        "merged_2022_data.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Combined 2022 data for London boroughs:\")\n",
        "merged_2022_data.head()"
      ],
      "metadata": {
        "id": "kddCKw9BkwJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting non-numeric values like \"#\" and \"!\" in income dataframe to NaN values and then replacing them with the mean income to make it suitable for analysis.\n",
        "merged_2022_data['2022_Avg_Weekly_Earnings'] = pd.to_numeric(merged_2022_data['2022_Avg_Weekly_Earnings'], errors='coerce')\n",
        "mean_earnings = merged_2022_data['2022_Avg_Weekly_Earnings'].mean()\n",
        "merged_2022_data['2022_Avg_Weekly_Earnings'].fillna(mean_earnings, inplace=True)\n",
        "\n",
        "print(\"Cleaned 2022_Avg_Weekly_Earnings column:\")\n",
        "print(merged_2022_data[['Local Authority', '2022_Avg_Weekly_Earnings']].head())"
      ],
      "metadata": {
        "id": "zi06jYFXk0in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_2022_data['2022_Avg_Weekly_Earnings'] = pd.to_numeric(merged_2022_data['2022_Avg_Weekly_Earnings'], errors='coerce')\n",
        "mean_earnings = merged_2022_data['2022_Avg_Weekly_Earnings'].mean()\n",
        "merged_2022_data['2022_Avg_Weekly_Earnings'] = merged_2022_data['2022_Avg_Weekly_Earnings'].fillna(mean_earnings)\n",
        "\n",
        "print(\"Cleaned 2022_Avg_Weekly_Earnings column:\")\n",
        "print(merged_2022_data[['Local Authority', '2022_Avg_Weekly_Earnings']].head())"
      ],
      "metadata": {
        "id": "0p_A7obrk83M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the descriptive statistics for the merged 2022 data\n",
        "print(\"Descriptive statistics for combined 2022 data:\")\n",
        "print(merged_2022_data.describe())"
      ],
      "metadata": {
        "id": "hKauGKFpk_j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the merged dataframe\n",
        "print(\"Final merged 2022 data:\")\n",
        "display(merged_2022_data)"
      ],
      "metadata": {
        "id": "1gRwl54hlCA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running an Ordinary Least Squares (OLS) regression by importing the \"statsmodels\" library.\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Defining the OLS model using the specified formula and data\n",
        "model = smf.ols('Q(\"2022 PM2.5\") ~ Q(\"2022_Vehicle_Traffic\") + Q(\"2022_Avg_Weekly_Earnings\") + Q(\"2022_Population_Density\")', data=merged_2022_data)\n",
        "\n",
        "# Fitting the OLS model\n",
        "results = model.fit()\n",
        "\n",
        "print(\"OLS model fitted successfully.\")"
      ],
      "metadata": {
        "id": "B5SLTf3NlGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the OLS Regression Results\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "x5dNt8KJlKEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating scatter plots for each independent variable against air pollution with regression lines\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot 1: 2022 PM2.5 vs. 2022_Vehicle_Traffic\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='2022_Vehicle_Traffic', y='2022 PM2.5', data=merged_2022_data, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
        "plt.title('2022 PM2.5 vs. 2022 Vehicle Traffic with Regression Line')\n",
        "plt.xlabel('2022 Vehicle Traffic (All vehicles)')\n",
        "plt.ylabel('2022 PM2.5 (tonnes/annum)')\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: 2022 PM2.5 vs. 2022_Avg_Weekly_Earnings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='2022_Avg_Weekly_Earnings', y='2022 PM2.5', data=merged_2022_data, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
        "plt.title('2022 PM2.5 vs. 2022 Average Weekly Earnings with Regression Line')\n",
        "plt.xlabel('2022 Average Weekly Earnings (£)')\n",
        "plt.ylabel('2022 PM2.5 (tonnes/annum)')\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: 2022 PM2.5 vs. 2022_Population_Density\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='2022_Population_Density', y='2022 PM2.5', data=merged_2022_data, scatter_kws={'alpha':0.6}, line_kws={'color':'red'})\n",
        "plt.title('2022 PM2.5 vs. 2022 Population Density with Regression Line')\n",
        "plt.xlabel('2022 Population Density (people per sq. km)')\n",
        "plt.ylabel('2022 PM2.5 (tonnes/annum)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V8cZsl1qlNU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Checking for multicollinearity\n",
        "sns.heatmap(merged_2022_data[['2022_Vehicle_Traffic', '2022_Avg_Weekly_Earnings', '2022_Population_Density']].corr(), # plot a correlation matrix\n",
        "            annot=True, # show the correlation values on the plot\n",
        "            fmt=\".2f\", # set the format of the correlation values to be two decimal places\n",
        "            cmap='coolwarm') # set the color palette to be coolwarm (blue for negative correlations, red for positive correlations)\n",
        "\n",
        "plt.title('Correlation Matrix of Independent Variables (2022)') # add a title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qrEIww8Sv2FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating VIF\n",
        "# This function is amended from: https://stackoverflow.com/a/51329496/4667568\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "def drop_column_using_vif_(df, list_var_not_to_remove=None, thresh=5):\n",
        "    '''\n",
        "    Calculates VIF each feature in a pandas dataframe, and repeatedly drop the columns with the highest VIF\n",
        "    A constant must be added to variance_inflation_factor or the results will be incorrect\n",
        "\n",
        "    :param df: the pandas dataframe containing only the predictor features, not the response variable\n",
        "    :param list_var_not_to_remove: the list of variables that should not be removed even though it has a high VIF. For example, dummy (or indicator) variables represent a categorical variable with three or more categories.\n",
        "    :param thresh: the max VIF value before the feature is removed from the dataframe\n",
        "    :return: dataframe with multicollinear features removed\n",
        "    '''\n",
        "    while True:\n",
        "        # adding a constatnt item to the data\n",
        "        df_with_const = add_constant(df)\n",
        "\n",
        "        vif_df = pd.Series([variance_inflation_factor(df_with_const.values, i)\n",
        "               for i in range(df_with_const.shape[1])], name= \"VIF\",\n",
        "              index=df_with_const.columns).to_frame()\n",
        "\n",
        "        # drop the const as const should not be removed\n",
        "        vif_df = vif_df.drop('const')\n",
        "\n",
        "        # drop the variables that should not be removed\n",
        "        if list_var_not_to_remove is not None:\n",
        "            vif_df = vif_df.drop(list_var_not_to_remove)\n",
        "\n",
        "        print('Max VIF:', vif_df.VIF.max())\n",
        "\n",
        "        # if the largest VIF is above the thresh, remove a variable with the largest VIF\n",
        "        if vif_df.VIF.max() > thresh:\n",
        "            # If there are multiple variables with the maximum VIF, choose the first one\n",
        "            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]\n",
        "            print('Dropping: {}'.format(index_to_drop))\n",
        "            df = df.drop(columns = index_to_drop)\n",
        "        else:\n",
        "            # No VIF is above threshold. Exit the loop\n",
        "            break\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Y-SgMuYrxLVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af98ef12"
      },
      "source": [
        "ind_vars=['2022_Vehicle_Traffic', '2022_Avg_Weekly_Earnings', '2022_Population_Density']\n",
        "X = merged_2022_data[ind_vars].copy()\n",
        "X.dropna(inplace=True)\n",
        "vif = drop_column_using_vif_(X, thresh=5)\n",
        "print(\"The columns remaining after VIF selection are:\")\n",
        "print(vif.columns)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}